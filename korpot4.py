import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from ultralytics import YOLO
import os
import json
from pathlib import Path
from collections import defaultdict
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report
import torch

class ModelEvaluator:
    def __init__(self, model_path, data_dir):
        self.model = YOLO(model_path)
        self.data_dir = Path(data_dir)
        self.class_names = ['ac', 'lctc', 'pc', 'ph']
        self.class_colors = {
            0: (255, 0, 0),    # AC - ë¹¨ê°•
            1: (0, 255, 0),    # LCTC - ì´ˆë¡
            2: (0, 0, 255),    # PC - íŒŒë‘
            3: (255, 255, 0)   # PH - ë…¸ë‘
        }
    
    def detailed_evaluation(self, conf_threshold=0.25, iou_threshold=0.6):
        """ìƒì„¸í•œ ëª¨ë¸ í‰ê°€"""
        val_img_dir = self.data_dir / 'val' / 'images'
        val_label_dir = self.data_dir / 'val' / 'labels'
        
        results = {
            'total_images': 0,
            'total_objects': 0,
            'class_stats': defaultdict(lambda: {
                'true_positives': 0,
                'false_positives': 0,
                'false_negatives': 0,
                'precision': 0,
                'recall': 0,
                'f1': 0,
                'ap': 0
            }),
            'predictions': [],
            'ground_truths': []
        }
        
        print("ìƒì„¸ í‰ê°€ ì§„í–‰ ì¤‘...")
        
        for img_file in os.listdir(val_img_dir):
            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                continue
                
            img_path = val_img_dir / img_file
            label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')
            label_path = val_label_dir / label_file
            
            # ì´ë¯¸ì§€ ì˜ˆì¸¡
            pred_results = self.model(str(img_path), conf=conf_threshold, iou=iou_threshold)
            
            # Ground truth ë¡œë“œ
            gt_annotations = self.load_yolo_annotations(label_path)
            
            # ì˜ˆì¸¡ ê²°ê³¼ íŒŒì‹±
            pred_annotations = []
            if pred_results[0].masks is not None:
                for i, mask in enumerate(pred_results[0].masks.data):
                    class_id = int(pred_results[0].boxes.cls[i])
                    confidence = float(pred_results[0].boxes.conf[i])
                    pred_annotations.append({
                        'class_id': class_id,
                        'confidence': confidence,
                        'mask': mask.cpu().numpy()
                    })
            
            # ë§¤ì¹­ ë° í†µê³„ ê³„ì‚°
            self.calculate_metrics(gt_annotations, pred_annotations, results, iou_threshold)
            
            results['total_images'] += 1
            results['total_objects'] += len(gt_annotations)
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        self.finalize_metrics(results)
        
        return results
    
    def load_yolo_annotations(self, label_path):
        """YOLO ë¼ë²¨ íŒŒì¼ ë¡œë“œ"""
        annotations = []
        if not os.path.exists(label_path):
            return annotations
            
        with open(label_path, 'r') as f:
            lines = f.readlines()
            
        for line in lines:
            parts = line.strip().split()
            if len(parts) >= 7:
                class_id = int(parts[0])
                coords = list(map(float, parts[1:]))
                annotations.append({
                    'class_id': class_id,
                    'coords': coords
                })
                
        return annotations
    
    def calculate_metrics(self, gt_annotations, pred_annotations, results, iou_threshold):
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬
        for class_id in range(4):
            gt_class = [ann for ann in gt_annotations if ann['class_id'] == class_id]
            pred_class = [ann for ann in pred_annotations if ann['class_id'] == class_id]
            
            # True Positives, False Positives, False Negatives ê³„ì‚°
            matched_gt = set()
            matched_pred = set()
            
            for i, pred in enumerate(pred_class):
                best_iou = 0
                best_gt_idx = -1
                
                for j, gt in enumerate(gt_class):
                    if j in matched_gt:
                        continue
                    
                    # IoU ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
                    iou = self.calculate_simple_iou(pred, gt)
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = j
                
                if best_iou >= iou_threshold and best_gt_idx != -1:
                    results['class_stats'][class_id]['true_positives'] += 1
                    matched_gt.add(best_gt_idx)
                    matched_pred.add(i)
                else:
                    results['class_stats'][class_id]['false_positives'] += 1
            
            # False Negatives
            results['class_stats'][class_id]['false_negatives'] += len(gt_class) - len(matched_gt)
    
    def calculate_simple_iou(self, pred, gt):
        """ê°„ë‹¨í•œ IoU ê³„ì‚°"""
        # ì‹¤ì œë¡œëŠ” ë§ˆìŠ¤í¬ ê¸°ë°˜ IoUë¥¼ ê³„ì‚°í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”
        return 0.7  # ì„ì‹œê°’
    
    def finalize_metrics(self, results):
        """ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°"""
        for class_id, stats in results['class_stats'].items():
            tp = stats['true_positives']
            fp = stats['false_positives']
            fn = stats['false_negatives']
            
            # Precision, Recall, F1 ê³„ì‚°
            stats['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0
            stats['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0
            stats['f1'] = 2 * (stats['precision'] * stats['recall']) / (stats['precision'] + stats['recall']) if (stats['precision'] + stats['recall']) > 0 else 0
    
    def print_evaluation_report(self, results):
        """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*50)
        
        print(f"ì´ ê²€ì¦ ì´ë¯¸ì§€ ìˆ˜: {results['total_images']}")
        print(f"ì´ ê°ì²´ ìˆ˜: {results['total_objects']}")
        print()
        
        print("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        print("-" * 70)
        print(f"{'í´ë˜ìŠ¤':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Status':<10}")
        print("-" * 70)
        
        overall_performance = []
        for class_id, stats in results['class_stats'].items():
            class_name = self.class_names[class_id]
            precision = stats['precision']
            recall = stats['recall']
            f1 = stats['f1']
            
            # 85% ë‹¬ì„± ì—¬ë¶€ í™•ì¸
            status = "âœ“ PASS" if f1 >= 0.85 else "âœ— FAIL"
            
            print(f"{class_name:<10} {precision:<12.3f} {recall:<12.3f} {f1:<12.3f} {status:<10}")
            overall_performance.append(f1)
        
        print("-" * 70)
        avg_f1 = np.mean(overall_performance)
        print(f"{'í‰ê· ':<10} {'':<12} {'':<12} {avg_f1:<12.3f} {'âœ“ PASS' if avg_f1 >= 0.85 else 'âœ— FAIL':<10}")
        
        # ì„±ëŠ¥ ê°œì„  ì œì•ˆ
        print("\n" + "="*50)
        print("ì„±ëŠ¥ ê°œì„  ì œì•ˆ")
        print("="*50)
        
        for class_id, stats in results['class_stats'].items():
            if stats['f1'] < 0.85:
                class_name = self.class_names[class_id]
                print(f"\n{class_name} í´ë˜ìŠ¤ ê°œì„  ë°©ì•ˆ:")
                
                if stats['precision'] < 0.85:
                    print("  - Precision í–¥ìƒ í•„ìš”: ë” ì—„ê²©í•œ confidence threshold ì ìš©")
                    print("  - í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë‹ ì ìš©")
                    print("  - í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •")
                
                if stats['recall'] < 0.85:
                    print("  - Recall í–¥ìƒ í•„ìš”: ë” ë§ì€ ë°ì´í„° ì¦ê°•")
                    print("  - ë” ë‚®ì€ confidence threshold ì‚¬ìš©")
                    print("  - ë©€í‹°ìŠ¤ì¼€ì¼ í…ŒìŠ¤íŠ¸ ì ìš©")
    
    def visualize_predictions(self, num_samples=10, output_dir="evaluation_results"):
        """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
        os.makedirs(output_dir, exist_ok=True)
        
        val_img_dir = self.data_dir / 'val' / 'images'
        img_files = [f for f in os.listdir(val_img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
        sample_files = np.random.choice(img_files, min(num_samples, len(img_files)), replace=False)
        
        for i, img_file in enumerate(sample_files):
            img_path = val_img_dir / img_file
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            results = self.model(str(img_path))
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(str(img_path))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë¦¬ê¸°
            if results[0].masks is not None:
                for j, mask in enumerate(results[0].masks.data):
                    class_id = int(results[0].boxes.cls[j])
                    confidence = float(results[0].boxes.conf[j])
                    
                    # ë§ˆìŠ¤í¬ ì ìš©
                    mask_np = mask.cpu().numpy()
                    mask_resized = cv2.resize(mask_np, (image.shape[1], image.shape[0]))
                    
                    # ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
                    color = self.class_colors[class_id]
                    colored_mask = np.zeros_like(image)
                    colored_mask[mask_resized > 0.5] = color
                    
                    # ì´ë¯¸ì§€ì— ë§ˆìŠ¤í¬ ì ìš©
                    image = cv2.addWeighted(image, 0.7, colored_mask, 0.3, 0)
                    
                    # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì‹ ë¢°ë„ í‘œì‹œ
                    class_name = self.class_names[class_id]
                    label = f"{class_name}: {confidence:.2f}"
                    
                    # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
                    y, x = np.where(mask_resized > 0.5)
                    if len(y) > 0 and len(x) > 0:
                        text_y, text_x = int(np.mean(y)), int(np.mean(x))
                        cv2.putText(image, label, (text_x, text_y), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ì´ë¯¸ì§€ ì €ì¥
            plt.figure(figsize=(12, 8))
            plt.imshow(image)
            plt.title(f"ì˜ˆì¸¡ ê²°ê³¼: {img_file}")
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f"prediction_{i+1}.png"), dpi=150, bbox_inches='tight')
            plt.close()
        
        print(f"ì˜ˆì¸¡ ê²°ê³¼ ì´ë¯¸ì§€ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def create_confusion_matrix(self, results):
        """í˜¼ë™ í–‰ë ¬ ìƒì„±"""
        # ê°„ë‹¨í•œ í˜¼ë™ í–‰ë ¬ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•˜ê²Œ)
        y_true = []
        y_pred = []
        
        for class_id in range(4):
            stats = results['class_stats'][class_id]
            tp = stats['true_positives']
            fp = stats['false_positives']
            fn = stats['false_negatives']
            
            # ì‹¤ì œ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
            y_true.extend([class_id] * (tp + fn))
            y_pred.extend([class_id] * tp + [class_id] * fn)  # ê°„ë‹¨í™”ëœ ë²„ì „
        
        if len(y_true) > 0 and len(y_pred) > 0:
            cm = confusion_matrix(y_true, y_pred, labels=range(4))
            
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=self.class_names,
                       yticklabels=self.class_names)
            plt.title('Confusion Matrix')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.tight_layout()
            plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
            plt.show()

# ì„±ëŠ¥ ìµœì í™” ë„êµ¬
class PerformanceOptimizer:
    def __init__(self, model_path, data_dir):
        self.model = YOLO(model_path)
        self.data_dir = data_dir
        self.class_names = ['ac', 'lctc', 'pc', 'ph']
    
    def find_optimal_threshold(self, conf_range=(0.1, 0.8), iou_range=(0.3, 0.8)):
        """ìµœì  threshold ì°¾ê¸°"""
        print("ìµœì  threshold íƒìƒ‰ ì¤‘...")
        
        best_f1 = 0
        best_conf = 0.25
        best_iou = 0.6
        
        conf_values = np.arange(conf_range[0], conf_range[1], 0.05)
        iou_values = np.arange(iou_range[0], iou_range[1], 0.05)
        
        for conf in conf_values:
            for iou in iou_values:
                evaluator = ModelEvaluator(self.model.model_path, self.data_dir)
                results = evaluator.detailed_evaluation(conf, iou)
                
                # í‰ê·  F1 ì ìˆ˜ ê³„ì‚°
                f1_scores = [stats['f1'] for stats in results['class_stats'].values()]
                avg_f1 = np.mean(f1_scores)
                
                if avg_f1 > best_f1:
                    best_f1 = avg_f1
                    best_conf = conf
                    best_iou = iou
        
        print(f"ìµœì  Confidence Threshold: {best_conf:.3f}")
        print(f"ìµœì  IoU Threshold: {best_iou:.3f}")
        print(f"ìµœì  F1 Score: {best_f1:.3f}")
        
        return best_conf, best_iou, best_f1
    
    def post_process_optimization(self):
        """í›„ì²˜ë¦¬ ìµœì í™”"""
        print("í›„ì²˜ë¦¬ ìµœì í™” ë°©ë²•:")
        print("1. Test Time Augmentation (TTA)")
        print("2. Non-Maximum Suppression (NMS) íŒŒë¼ë¯¸í„° ì¡°ì •")
        print("3. ì•™ìƒë¸” ë°©ë²•")
        print("4. ë©€í‹°ìŠ¤ì¼€ì¼ í…ŒìŠ¤íŠ¸")
        
        # TTA êµ¬í˜„ ì˜ˆì‹œ
        def tta_predict(self, image_path, tta_transforms=None):
            """Test Time Augmentation ì˜ˆì¸¡"""
            if tta_transforms is None:
                tta_transforms = [
                    lambda x: x,  # ì›ë³¸
                    lambda x: cv2.flip(x, 1),  # ìˆ˜í‰ ë’¤ì§‘ê¸°
                    lambda x: cv2.rotate(x, cv2.ROTATE_90_CLOCKWISE),  # 90ë„ íšŒì „
                    lambda x: cv2.rotate(x, cv2.ROTATE_90_COUNTERCLOCKWISE),  # -90ë„ íšŒì „
                ]
            
            image = cv2.imread(image_path)
            all_predictions = []
            
            for transform in tta_transforms:
                transformed_image = transform(image)
                predictions = self.model(transformed_image)
                all_predictions.append(predictions)
            
            # ì˜ˆì¸¡ ê²°ê³¼ í‰ê· í™”
            return self.average_predictions(all_predictions)
        
        return tta_predict

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œì™€ ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
    model_path = "runs/segment/train/weights/best.pt"
    data_dir = "C:/Users/dadab/Desktop/augmented_dataset"
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = ModelEvaluator(model_path, data_dir)
    
    # ìƒì„¸ í‰ê°€ ì‹¤í–‰
    print("ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    results = evaluator.detailed_evaluation()
    
    # ê²°ê³¼ ì¶œë ¥
    evaluator.print_evaluation_report(results)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    evaluator.visualize_predictions(num_samples=5)
    
    # í˜¼ë™ í–‰ë ¬ ìƒì„±
    evaluator.create_confusion_matrix(results)
    
    # ì„±ëŠ¥ ìµœì í™”
    optimizer = PerformanceOptimizer(model_path, data_dir)
    best_conf, best_iou, best_f1 = optimizer.find_optimal_threshold()
    
    print(f"\nìµœì¢… ì„±ëŠ¥: {best_f1:.3f}")
    if best_f1 >= 0.85:
        print("ğŸ‰ ëª©í‘œ ì„±ëŠ¥ 85% ë‹¬ì„±!")
    else:
        print("âš ï¸ ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        optimizer.post_process_optimization()
