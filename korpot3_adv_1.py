import os
import yaml
import torch
import numpy as np
from ultralytics import YOLO
from pathlib import Path
import cv2
import time
from datetime import datetime

class YOLOv8SegmentationTrainer:
    def __init__(self, data_dir, model_name="yolov8s-seg.pt"):
        self.data_dir = Path(data_dir)
        self.model_name = model_name
        self.class_names = ['ac', 'lctc', 'pc', 'ph']
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™” ë° ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"ğŸš€ GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    def create_data_yaml(self, output_path="dataset.yaml"):
        data_config = {
            'path': str(self.data_dir.absolute()),
            'train': 'train/images',
            'val': 'val/images',
            'nc': len(self.class_names),
            'names': self.class_names
        }
        with open(output_path, 'w') as f:
            yaml.dump(data_config, f, default_flow_style=False)
        return output_path

    def get_optimal_settings(self):
        """RTX 3070 + ë„ë¡œ ê²°í•¨ ê°ì§€ì— ìµœì í™”ëœ ì„¤ì • (ì†ë„ ê°œì„ )"""
        return {
            # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
            'epochs': 200,          # 300 â†’ 200 (ì¡°ê¸° ì¢…ë£Œ ê³ ë ¤)
            'imgsz': 640,
            'batch': 16,            # 12 â†’ 16 (GPU í™œìš©ë„ ì¦ê°€)
            'workers': 2,           # 0 â†’ 2 (ì•ˆì „í•œ ë©€í‹°í”„ë¡œì„¸ì‹±)
            'patience': 20,         # 25 â†’ 20 (ë¹ ë¥¸ ìˆ˜ë ´)
            
            # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”
            'cache': False,         # ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ìºì‹œ ë¹„í™œì„±í™”
            'amp': True,
            'multi_scale': False,   # True â†’ False (ì†ë„ í–¥ìƒ)
            'overlap_mask': True,
            'mask_ratio': 4,
            'close_mosaic': 15,     # 10 â†’ 15 (ì•ˆì •ì„±)
            
            # ì˜µí‹°ë§ˆì´ì € (85% mAP ëª©í‘œ)
            'optimizer': 'AdamW',
            'lr0': 0.002,
            'lrf': 0.0001,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 3.0,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            
            # Loss ê°€ì¤‘ì¹˜ (ì„¸ê·¸ë©˜í…Œì´ì…˜ ìµœì í™”)
            'box': 7.5,
            'cls': 0.5,
            'dfl': 1.5,
            
            # ë°ì´í„° ì¦ê°• (ì†ë„ ìµœì í™”)
            'hsv_h': 0.005,         # 0.008 â†’ 0.005 (ì—°ì‚° ê°ì†Œ)
            'hsv_s': 0.2,           # 0.3 â†’ 0.2
            'hsv_v': 0.1,           # 0.15 â†’ 0.1
            'degrees': 2.0,         # 3.0 â†’ 2.0
            'translate': 0.02,      # 0.03 â†’ 0.02
            'scale': 0.15,          # 0.2 â†’ 0.15
            'shear': 0.0,
            'perspective': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 0.7,          # 0.9 â†’ 0.7 (ì†ë„ í–¥ìƒ)
            'mixup': 0.0,           # 0.05 â†’ 0.0 (ë¹„í™œì„±í™”)
            'copy_paste': 0.0,
            
            # ìŠ¤ì¼€ì¤„ëŸ¬
            'cos_lr': True,
            
            # ì €ì¥ ë° ê²€ì¦
            'save': True,
            'save_period': 20,
            'val': True,
            'plots': True,
            'verbose': True
        }

    def check_dataset_info(self):
        """ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸"""
        train_dir = self.data_dir / 'train' / 'images'
        val_dir = self.data_dir / 'val' / 'images'
        
        train_count = len(list(train_dir.glob("*.jpg"))) + len(list(train_dir.glob("*.png")))
        val_count = len(list(val_dir.glob("*.jpg"))) + len(list(val_dir.glob("*.png")))
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   í•™ìŠµ: {train_count}ì¥")
        print(f"   ê²€ì¦: {val_count}ì¥")
        print(f"   í´ë˜ìŠ¤: {self.class_names}")
        
        return train_count, val_count

    def train_model(self):
        print(f"ğŸ¯ ëª¨ë¸: {self.model_name}")
        self.check_dataset_info()
        
        model = YOLO(self.model_name)
        data_yaml = self.create_data_yaml()
        args = self.get_optimal_settings()
        
        args.update({
            'data': data_yaml,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'project': 'runs/segment',
            'name': f'train_{self.data_dir.name}_{datetime.now().strftime("%m%d_%H%M")}'
        })
        
        print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ì´ë¯¸ì§€ í¬ê¸°: {args['imgsz']}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {args['batch']}")
        print(f"   í•™ìŠµë¥ : {args['lr0']}")
        print(f"   ì›Œì»¤ ìˆ˜: {args['workers']} (Windows í˜¸í™˜ ëª¨ë“œ)")
        
        start_time = time.time()
        results = model.train(**args)
        training_time = time.time() - start_time
        
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {training_time/3600:.1f}ì‹œê°„")
        
        return model, results

    def validate_model(self, model_path=None):
        if model_path:
            model = YOLO(model_path)
        else:
            # ê°€ì¥ ìµœê·¼ í•™ìŠµëœ ëª¨ë¸ ì°¾ê¸°
            runs_dir = Path('runs/segment')
            if runs_dir.exists():
                latest_run = max([d for d in runs_dir.iterdir() if d.is_dir()], 
                               key=os.path.getctime)
                model = YOLO(latest_run / 'weights' / 'best.pt')
                print(f"ğŸ” ëª¨ë¸ ë¡œë“œ: {latest_run / 'weights' / 'best.pt'}")
            else:
                print("âŒ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

        data_yaml = self.create_data_yaml()
        results = model.val(
            data=data_yaml,
            imgsz=640,
            batch=12,
            conf=0.25,
            iou=0.6,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )

        print("\nğŸ“Š ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ëŠ¥ ê²°ê³¼:")
        print("=" * 55)
        
        try:
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ìš°ì„  í™•ì¸
            if hasattr(results, 'seg') and results.seg is not None:
                maps_50 = results.seg.map50
                maps_5095 = results.seg.map
                print("ğŸ¯ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë©”íŠ¸ë¦­ ì‚¬ìš©")
            else:
                maps_50 = results.box.map50
                maps_5095 = results.box.map
                print("ğŸ“¦ ë°”ìš´ë”©ë°•ìŠ¤ ë©”íŠ¸ë¦­ ì‚¬ìš©")

            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì¶œë ¥
            success_count = 0
            for i, name in enumerate(self.class_names):
                if isinstance(maps_50, (list, np.ndarray)) and len(maps_50) > i:
                    m50 = maps_50[i]
                    m5095 = maps_5095[i] if isinstance(maps_5095, (list, np.ndarray)) else maps_5095
                else:
                    m50 = maps_50 if not isinstance(maps_50, (list, np.ndarray)) else 0
                    m5095 = maps_5095 if not isinstance(maps_5095, (list, np.ndarray)) else 0
                
                status = "âœ…" if m50 >= 0.85 else "âŒ"
                if m50 >= 0.85:
                    success_count += 1
                    
                print(f"{name:>4} â†’ mAP@0.5: {m50:.3f} | mAP@0.5:0.95: {m5095:.3f} {status}")

            # ì „ì²´ í‰ê·  ë° ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
            avg_map50 = np.mean(maps_50) if isinstance(maps_50, (list, np.ndarray)) else maps_50
            avg_map5095 = np.mean(maps_5095) if isinstance(maps_5095, (list, np.ndarray)) else maps_5095
            
            print("-" * 55)
            print(f"ğŸ“ˆ ì „ì²´ í‰ê·  mAP@0.5    : {avg_map50:.3f}")
            print(f"ğŸ“ˆ ì „ì²´ í‰ê·  mAP@0.5:0.95: {avg_map5095:.3f}")
            
            if success_count == len(self.class_names):
                print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! ëª¨ë“  í´ë˜ìŠ¤ê°€ 85% ì´ìƒ ë‹¬ì„± ({success_count}/{len(self.class_names)})")
            elif avg_map50 >= 0.85:
                print(f"\nğŸ¯ í‰ê·  ëª©í‘œ ë‹¬ì„±! í‰ê·  mAP@0.5: {avg_map50:.3f}")
                print(f"   ê°œë³„ ë‹¬ì„±: {success_count}/{len(self.class_names)} í´ë˜ìŠ¤")
            else:
                print(f"\nğŸ“Š í˜„ì¬ ìƒíƒœ: í‰ê·  mAP@0.5: {avg_map50:.3f} (ëª©í‘œ: 0.85)")
                print(f"   ê°œë³„ ë‹¬ì„±: {success_count}/{len(self.class_names)} í´ë˜ìŠ¤")
                
        except Exception as e:
            print("âš ï¸ í‰ê°€ ì§€í‘œ ì¶”ì¶œ ì‹¤íŒ¨:", e)
            print("ê²°ê³¼ ê°ì²´ êµ¬ì¡°:", dir(results))

        return results

    def visualize_predictions(self, model_path=None, num_samples=10):
        if model_path:
            model = YOLO(model_path)
        else:
            runs_dir = Path('runs/segment')
            if runs_dir.exists():
                latest_run = max([d for d in runs_dir.iterdir() if d.is_dir()], 
                               key=os.path.getctime)
                model = YOLO(latest_run / 'weights' / 'best.pt')
            else:
                print("âŒ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

        image_dir = self.data_dir / 'val' / 'images'
        save_dir = Path("inference_samples")
        save_dir.mkdir(parents=True, exist_ok=True)

        images = list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.png"))
        selected_images = images[:num_samples] if len(images) >= num_samples else images

        print(f"ğŸ–¼ï¸ {len(selected_images)}ì¥ ì´ë¯¸ì§€ ì¶”ë¡  ì¤‘...")
        
        for i, img_path in enumerate(selected_images):
            results = model(img_path, save=False, stream=False, conf=0.25, iou=0.6)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ì— í´ë˜ìŠ¤ ì •ë³´ ì¶”ê°€
            result_img = results[0].plot()
            
            # íŒŒì¼ëª…ì— ì›ë³¸ ì´ë¦„ í¬í•¨
            original_name = img_path.stem
            save_path = save_dir / f"result_{i:02d}_{original_name}.jpg"
            cv2.imwrite(str(save_path), result_img)
            
            # ê°„ë‹¨í•œ ì§„í–‰ìƒí™© í‘œì‹œ
            if (i + 1) % 5 == 0 or i == len(selected_images) - 1:
                print(f"   ì§„í–‰: {i+1}/{len(selected_images)}")

        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_dir}")

    def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ YOLOv8 ì„¸ê·¸ë©˜í…Œì´ì…˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)
        
        # 1. í•™ìŠµ
        print("\n1ï¸âƒ£ í•™ìŠµ ë‹¨ê³„")
        model, train_results = self.train_model()
        
        # 2. ê²€ì¦
        print("\n2ï¸âƒ£ ê²€ì¦ ë‹¨ê³„")
        val_results = self.validate_model()
        
        # 3. ì‹œê°í™”
        print("\n3ï¸âƒ£ ì‹œê°í™” ë‹¨ê³„")
        self.visualize_predictions(num_samples=15)
        
        print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        return model, train_results, val_results

if __name__ == "__main__":
    # Windowsì—ì„œ multiprocessing ì˜¤ë¥˜ ë°©ì§€
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    
    data_dir = "C:/Users/dadab/Desktop/augmented_dataset"
    trainer = YOLOv8SegmentationTrainer(data_dir, model_name="yolov8s-seg.pt")
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    trainer.run_full_pipeline()
